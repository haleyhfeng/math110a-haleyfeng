{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm, solve, multi_dot, inv\n",
    "from scipy.optimize import minimize_scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exact line search steepest descent method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_steepest_descent(x0):             # input is the starting point\n",
    "    x = x0                                  # select the starting point\n",
    "    p = -grad_objective_func(x)             # find descent direction\n",
    "    i = 1                                   # starting counter for iteration\n",
    "    while norm(p) > 1e-9:                   # if the norm is not small\n",
    "        def subproblem1d(alpha):            # define a 1d subproblem \n",
    "            return objective_func(x + alpha * p)  \n",
    "                                            # use minimize_scalar function\n",
    "        res = minimize_scalar(subproblem1d) # to solve the subproblem\n",
    "        x = x + res.x * p                   # locate the next iterate\n",
    "        p = -grad_objective_func(x)         # find next descent direction\n",
    "        i += 1\n",
    "    return i,x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed step length steepest descent method with α = 10−3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inexact_steepest_descent(x0):           # input is the starting point\n",
    "    x = x0                                  # select the starting point\n",
    "    p = -grad_objective_func(x)             # find descent direction\n",
    "    i = 1                                   # starting counter for iteration\n",
    "    while norm(p) > 1e-9:                   # if the norm is not small\n",
    "        x = x + 1e-3 * p                    # locate the next iterate\n",
    "        p = -grad_objective_func(x)         # find next descent direction\n",
    "        i += 1\n",
    "    return i, x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newton’s method with fixed step length as 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_method(x0):                      # input is the starting point\n",
    "    x = x0                                  # select the starting point\n",
    "    p = -grad_objective_func(x)             # find descent direction\n",
    "    h = hessian_func(x)                     # find hessian matrix\n",
    "    i = 0\n",
    "    while norm(p) > 1e-9:                   # if the norm is not small\n",
    "        newton_dir = np.linalg.solve(h, p)  # Newton direction\n",
    "        x = x + newton_dir                  # locate the next iterate\n",
    "        p = -grad_objective_func(x)         # find next descent direction\n",
    "        h = hessian_func(x)                 # find next hessian matrix\n",
    "        i += 1\n",
    "    return i, x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backtracking steepest descent method with c1 = 10−4, ρ = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtracking(objFunc, gradObjFunc, x0, pk, alpha0, c1, rho):\n",
    "    currentFuncValue = objFunc(x0)\n",
    "    currentGradient  = gradObjFunc(x0) \n",
    "    alpha            = alpha0 \n",
    "    # make sure it is a descent direction. \n",
    "    assert(np.dot(pk, currentGradient) < 0)\n",
    "    \n",
    "    while objFunc(x0 + alpha * pk) - currentFuncValue > c1 * alpha * np.dot(pk, currentGradient):\n",
    "        alpha = alpha * rho \n",
    "    \n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol     = 1e-9 \n",
    "maxIter = 1e6\n",
    "def backtracking_steepest_decent_method(objFunc, gradObjFunc, x0, tol, maxIter):\n",
    "    path      = [x0]\n",
    "    k         = 0\n",
    "    xk        = x0\n",
    "    pk        = -gradObjFunc(x0)\n",
    "    while norm(pk) > tol and k <= maxIter:\n",
    "        # at xk, find suitable alpha\n",
    "        alpha = backtracking(objFunc, gradObjFunc, xk, pk, 1, 1e-4 , 0.1)\n",
    "        xk  = xk + alpha * pk \n",
    "        pk  = -gradObjFunc(xk)\n",
    "        k   = k + 1\n",
    "        path.append(xk)\n",
    "\n",
    "    path = np.array(path) # convert to array     \n",
    "    return xk, k, path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backtracking Newton’s method with c1 = 10−4, ρ = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtracking_newton_method(objFunc, gradObjFunc, hessianFunc, x0, tol, maxIter):                      \n",
    "    path      = [x0]\n",
    "    k         = 0\n",
    "    xk        = x0    \n",
    "    pk        = -np.linalg.solve(hessianFunc(xk), gradObjFunc(xk))        \n",
    "    while norm(gradObjFunc(xk)) > tol and k <= maxIter:                  \n",
    "        alpha = backtracking(objFunc, gradObjFunc, xk, pk, 1, 1e-4, 0.9)      \n",
    "        xk  = xk + alpha * pk \n",
    "        pk  = -np.linalg.solve(hessianFunc(xk)  , gradObjFunc(xk))\n",
    "        k   = k + 1\n",
    "        path.append(xk)\n",
    "        \n",
    "    path = np.array(path) # convert to array        \n",
    "    return xk, k, path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heavyball method with α = 10−3 and β = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1e-3 # fixed step length line search.\n",
    "beta  = 0.9  # large beta means the ball is very \"heavy\".\n",
    "def heavyBall(objFunc, gradObjFunc, x0, tol, maxIter):\n",
    "    path      = [x0]\n",
    "    k         = 0\n",
    "    xk        = x0    \n",
    "    pk        = -gradObjFunc(xk)\n",
    "    # Compute the first step separately. \n",
    "    if norm(pk) < tol:\n",
    "        return xk, 0, path\n",
    "    else:\n",
    "        k = k + 1\n",
    "        xk = xk + alpha * pk \n",
    "        path.append(xk)\n",
    "    \n",
    "    # The rest of iterations\n",
    "    pk        = -gradObjFunc(xk)\n",
    "    \n",
    "    while norm(pk) > tol and k <= maxIter: \n",
    "        # use path[-2] since path[-1] is the xk\n",
    "        xk  = xk + alpha * pk + beta * (xk - path[-2])\n",
    "        pk  = -gradObjFunc(xk)\n",
    "        k   = k + 1\n",
    "        path.append(xk)\n",
    "        \n",
    "    path = np.array(path) # convert to array        \n",
    "    return xk, k, path          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trust region method with Cauchy point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x, p):\n",
    "    return objective_func(x) + np.dot(p, grad_objective_func(x)) + 0.5 * multi_dot([p.T, hessian_func(x), p])\n",
    "\n",
    "\n",
    "def rho(x, p):\n",
    "    return (objective_func(x) - objective_func(x + p)) / (model(x, np.array([0, 0]))  - model(x, p))\n",
    "\n",
    "# framework for trust region method, we use the Hessian as the model's Bk.\n",
    "\n",
    "def trust_region(subproblem_solver, x0, radius0):\n",
    "    # The algorithm is modified from Algorithm 4.1 in the book.\n",
    "    xcoords = [x0[0]]\n",
    "    ycoords = [x0[1]]\n",
    "    iter    = 0\n",
    "    x = x0 \n",
    "    radius = radius0\n",
    "    err = norm(grad_objective_func(x))\n",
    "    while (err > 1e-9):\n",
    "        # main iterations for trust region.\n",
    "        p = subproblem_solver(x, radius)\n",
    "        # then test whether we should accept this point, calculate the ratio\n",
    "        r = rho(x, p)\n",
    "        if r < 0.25:\n",
    "            # reject this because it is too small, x is not updated, only radius is updated\n",
    "            radius = 0.5 * radius \n",
    "        elif r > 0.75:\n",
    "            # accept this \n",
    "            radius = min(2 * radius , radius0) # cannot exceed maximum region size\n",
    "            x = x + p\n",
    "        else:\n",
    "            # we also accept this, but region size is not changed\n",
    "            x = x + p\n",
    "            \n",
    "        xcoords.append(x[0])\n",
    "        ycoords.append(x[1])\n",
    "        iter = iter + 1\n",
    "            \n",
    "        # update err\n",
    "        err = norm(grad_objective_func(x))\n",
    "            \n",
    "    return x, iter, xcoords, ycoords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cauchy_point(x, radius):\n",
    "    grad = grad_objective_func(x)\n",
    "    hess = hessian_func(x)\n",
    "    if multi_dot([grad.T, hess, grad]) <= 0:\n",
    "        return -radius / norm(grad) * grad \n",
    "    else:\n",
    "        k = norm(grad)**3 / (radius * multi_dot([grad.T, hess, grad]))\n",
    "        return min(1, k) * (-radius / norm(grad) * grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trust region method with dogleg method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dogleg(x, radius):\n",
    "    grad = grad_objective_func(x)\n",
    "    hess = hessian_func(x)   \n",
    "    pn = -solve(hess, grad) \n",
    "    if norm(pn)<= radius:\n",
    "        return pn \n",
    "    else:\n",
    "        ps = -(norm(grad)**2)/(multi_dot([grad.T, hess, grad])) * grad\n",
    "        if norm(ps) <= radius:\n",
    "            A = norm(ps - pn)**2 \n",
    "            B = 2 * np.dot(ps, ps - pn)\n",
    "            C = norm(ps)**2 - radius**2 \n",
    "            t = (-B + np.sqrt(B**2 - 4 * A * C))/(2 * A)\n",
    "            return (1-t)*ps + t*pn\n",
    "        else:\n",
    "            return -radius / norm(grad) * grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BFGS method with exact line search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol = 1e-9\n",
    "\n",
    "def exact_line_search_quasi_newton(update_method, x0, H0):\n",
    "    k = 0\n",
    "    xcoords = [x0[0]]\n",
    "    ycoords = [x0[1]]\n",
    "    x_k = x0 \n",
    "    H_k = H0 \n",
    "    g_k = grad_objective_func(x_k)\n",
    "    while norm(g_k) > tol:\n",
    "        p_k = -np.matmul(H_k, g_k)                # search direction\n",
    "   \n",
    "        def subproblem1D(alpha):                  # for exact line search\n",
    "            return objective_func(x_k + alpha * p_k)\n",
    "        \n",
    "        res = minimize_scalar(subproblem1D)\n",
    "        alpha_k = res.x \n",
    "\n",
    "        s_k     = alpha_k * p_k                   # s_k = x_{k+1} - x_k \n",
    "        g_k1    = grad_objective_func(x_k + s_k)  # gk1 is g_{k+1}\n",
    "        y_k     = g_k1 - g_k                      # y_k = g_{k+1} - g_(k)\n",
    "        \n",
    "        k = k + 1                                 # increment\n",
    "        H_k = update_method(H_k, s_k, y_k).A      \n",
    "        x_k = x_k + s_k \n",
    "        g_k = g_k1\n",
    "        \n",
    "        xcoords.append(x_k[0])\n",
    "        ycoords.append(x_k[1])\n",
    "        \n",
    "    return x_k, k, norm(g_k), xcoords, ycoords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BFGS(H, s_k, y_k):\n",
    "    I = np.eye(H.shape[0])\n",
    "    y_vec = y_k[:, None] # want to vectorize for matrix multiplication\n",
    "    s_vec = s_k[:, None]\n",
    "    rho = 1/ np.dot(y_k,s_k) # this is a scalar so multiply with array form\n",
    "    l = I - rho*np.dot(s_vec, y_vec.T)\n",
    "    r = I - rho*np.dot(y_vec, s_vec.T)\n",
    "    H = multi_dot([l,H,r]) + rho*np.dot(s_vec,s_vec.T)\n",
    "    return np.matrix(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  SR1 method with exact line search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SR1(H, s_k, y_k):\n",
    "    z = s_k - np.dot(H, y_k) \n",
    "    # the numerator is a matrix!\n",
    "    numer = np.matrix(z).T * np.matrix(z)\n",
    "    denom = np.dot(z, y_k)\n",
    "    # when the denominator is too small, we skip the iteration.\n",
    "    if np.abs(denom) < 10**(-8) * norm(z) * norm(y_k):\n",
    "        return H\n",
    "    else:\n",
    "        return H + numer/denom "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BFGS and SR1 method with backtracking line search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtracking_line_search_quasi_newton(update_method, x0, H0):\n",
    "    k = 0\n",
    "    xcoords = [x0[0]]\n",
    "    ycoords = [x0[1]]\n",
    "    x_k = x0 \n",
    "    H_k = H0 \n",
    "    g_k = grad_objective_func(x_k)\n",
    "    while norm(g_k) > 1e-9:\n",
    "        p_k = -np.matmul(H_k, g_k)                # search direction\n",
    "        alpha = backtracking(objective_func, grad_objective_func, x_k, p_k, 1, 1e-4, 0.9)      \n",
    "        s_k     = alpha * p_k                     # s_k = x_{k+1} - x_k \n",
    "        g_k1    = grad_objective_func(x_k + s_k)  # gk1 is g_{k+1}\n",
    "        y_k     = g_k1 - g_k                      # y_k = g_{k+1} - g_(k)\n",
    "        \n",
    "        k = k + 1                                 # increment\n",
    "        H_k = update_method(H_k, s_k, y_k).A      \n",
    "        x_k = x_k + s_k \n",
    "        g_k = g_k1\n",
    "        \n",
    "        xcoords.append(x_k[0])\n",
    "        ycoords.append(x_k[1])\n",
    "        \n",
    "    return x_k, k, norm(g_k), xcoords, ycoords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Quadratic function, starting at (5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up objective and its gradient function\n",
    "def objective_func(x):\n",
    "    return (x[0]+2*x[1]-7)**2+(2*x[0]+x[1]-5)**2\n",
    "\n",
    "def grad_objective_func(x):\n",
    "    return np.array([10*x[0]+8*x[1]-34, 8*x[0]+10*x[1]-38])\n",
    "\n",
    "def hessian_func(x):\n",
    "    return np.matrix([[10, 8], [8, 10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration for exact steepest descent: 12\n",
      "Iteration for fixed step length steepest descent: 10872\n",
      "Iteration for newton's method: 1\n",
      "Iteration for backtracking steepest descent: 113\n",
      "Iteration for backtracking newton: 1\n",
      "Iteration for heavyball: 826\n",
      "Iteration for trust region w/ cauchy point: 77\n",
      "Iteration for trust region w/ dogleg: 3\n",
      "Iteration for BFGS w/ exact line search: 2\n",
      "Iteration for BFGS w/ backtracking: 6\n",
      "Iteration for SR1 w/ exact line search: 2\n",
      "Iteration for SR1 w/ backtracking: 3\n"
     ]
    }
   ],
   "source": [
    "x0 = np.array([5, 5])\n",
    "H0 = np.eye(2)\n",
    "i, x = exact_steepest_descent(x0)\n",
    "print(\"Iteration for exact steepest descent:\", i)\n",
    "i, x = inexact_steepest_descent(x0)\n",
    "print(\"Iteration for fixed step length steepest descent:\", i)\n",
    "i, x = newton_method(x0)\n",
    "print(\"Iteration for newton's method:\", i)\n",
    "xk, k, path = backtracking_steepest_decent_method(objective_func, grad_objective_func, x0, tol, maxIter)\n",
    "print(\"Iteration for backtracking steepest descent:\", k)\n",
    "xk, k, path = backtracking_newton_method(objective_func, grad_objective_func, hessian_func, x0, tol, maxIter)\n",
    "print(\"Iteration for backtracking newton:\", k)\n",
    "xk, k, path = heavyBall(objective_func, grad_objective_func, x0, tol, maxIter)\n",
    "print(\"Iteration for heavyball:\", k)\n",
    "x, iter, xcoords, ycoords  = trust_region(cauchy_point, x0, 2)\n",
    "print(\"Iteration for trust region w/ cauchy point:\", iter)\n",
    "x, iter, xcoords, ycoords  = trust_region(dogleg, x0, 2)\n",
    "print(\"Iteration for trust region w/ dogleg:\", iter)\n",
    "x, iter, err, xcoords, ycoords = exact_line_search_quasi_newton(BFGS, x0, H0)\n",
    "print(\"Iteration for BFGS w/ exact line search:\", iter)\n",
    "x, iter, err, xcoords, ycoords = backtracking_line_search_quasi_newton(BFGS, x0, H0)\n",
    "print(\"Iteration for BFGS w/ backtracking:\", iter)\n",
    "x, iter, err, xcoords, ycoords = exact_line_search_quasi_newton(SR1, x0, H0)\n",
    "print(\"Iteration for SR1 w/ exact line search:\", iter)\n",
    "x, iter, err, xcoords, ycoords = backtracking_line_search_quasi_newton(SR1, x0, H0)\n",
    "print(\"Iteration for SR1 w/ backtracking:\", iter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Rosenbrock function, starting at (−1.2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_func(x):\n",
    "    return 100 * (x[1] - x[0]**2)**2 + (1 - x[0])**2\n",
    "\n",
    "def grad_objective_func(x):\n",
    "    return np.array([-400 * x[0] * (x[1] - x[0] ** 2) - 2 * (1 - x[0]), 200 * (x[1] - x[0]**2)])\n",
    "\n",
    "def hessian_func(x):\n",
    "    return np.matrix([[-400*(x[1] - 3*x[0]**2) + 2, -400 * x[0]], [-400 * x[0], 200]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration for exact steepest descent: 24500\n",
      "Iteration for fixed step length steepest descent: 49371\n",
      "Iteration for newton's method: 7\n",
      "Iteration for backtracking steepest descent: 946\n",
      "Iteration for backtracking newton: 20\n",
      "Iteration for heavyball: 4682\n",
      "Iteration for trust region w/ cauchy point: 23176\n",
      "Iteration for trust region w/ dogleg: 38\n",
      "Iteration for BFGS w/ exact line search: 22\n",
      "Iteration for BFGS w/ backtracking: 21\n",
      "Iteration for SR1 w/ exact line search: 22\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-0c5421932cad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxcoords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mycoords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexact_line_search_quasi_newton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSR1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Iteration for SR1 w/ exact line search:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxcoords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mycoords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbacktracking_line_search_quasi_newton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSR1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Iteration for SR1 w/ backtracking:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-44eb5752f476>\u001b[0m in \u001b[0;36mbacktracking_line_search_quasi_newton\u001b[0;34m(update_method, x0, H0)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_k\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1e-9\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mp_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_k\u001b[0m\u001b[0;34m)\u001b[0m                \u001b[0;31m# search direction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbacktracking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_objective_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0ms_k\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mp_k\u001b[0m                     \u001b[0;31m# s_k = x_{k+1} - x_k\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mg_k1\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mgrad_objective_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_k\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms_k\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# gk1 is g_{k+1}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-4535e8374d7f>\u001b[0m in \u001b[0;36mbacktracking\u001b[0;34m(objFunc, gradObjFunc, x0, pk, alpha0, c1, rho)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0malpha\u001b[0m            \u001b[0;34m=\u001b[0m \u001b[0malpha0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# make sure it is a descent direction.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrentGradient\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mobjFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcurrentFuncValue\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mc1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrentGradient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x0 = np.array([-1.2, 1.])\n",
    "H0 = np.eye(2)\n",
    "\n",
    "i, x = exact_steepest_descent(x0)\n",
    "print(\"Iteration for exact steepest descent:\", i)\n",
    "i, x = inexact_steepest_descent(x0)\n",
    "print(\"Iteration for fixed step length steepest descent:\", i)\n",
    "i, x = newton_method(x0)\n",
    "print(\"Iteration for newton's method:\", i)\n",
    "xk, k, path = backtracking_steepest_decent_method(objective_func, grad_objective_func, x0, tol, maxIter)\n",
    "print(\"Iteration for backtracking steepest descent:\", k)\n",
    "xk, k, path = backtracking_newton_method(objective_func, grad_objective_func, hessian_func, x0, tol, maxIter)\n",
    "print(\"Iteration for backtracking newton:\", k)\n",
    "xk, k, path = heavyBall(objective_func, grad_objective_func, x0, tol, maxIter)\n",
    "print(\"Iteration for heavyball:\", k)\n",
    "x, iter, xcoords, ycoords  = trust_region(cauchy_point, x0, 2)\n",
    "print(\"Iteration for trust region w/ cauchy point:\", iter)\n",
    "x, iter, xcoords, ycoords  = trust_region(dogleg, x0, 2)\n",
    "print(\"Iteration for trust region w/ dogleg:\", iter)\n",
    "x, iter, err, xcoords, ycoords = exact_line_search_quasi_newton(BFGS, x0, H0)\n",
    "print(\"Iteration for BFGS w/ exact line search:\", iter)\n",
    "x, iter, err, xcoords, ycoords = backtracking_line_search_quasi_newton(BFGS, x0, H0)\n",
    "print(\"Iteration for BFGS w/ backtracking:\", iter)\n",
    "x, iter, err, xcoords, ycoords = exact_line_search_quasi_newton(SR1, x0, H0)\n",
    "print(\"Iteration for SR1 w/ exact line search:\", iter)\n",
    "x, iter, err, xcoords, ycoords = backtracking_line_search_quasi_newton(SR1, x0, H0)\n",
    "print(\"Iteration for SR1 w/ backtracking:\", iter)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: Powell’s quartic function, starting at (3, −1, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_func(x):\n",
    "    return (x[0] + 10*x[1])**2 + 5*(x[2] - x[3])**2 + (x[1] - 2*x[2])**4 + 10*(x[0] - x[3])**4\n",
    "\n",
    "def grad_objective_func(x):\n",
    "    return np.array([2*(x[0] + 10*x[1]) + 40*(x[0] - x[3])**3, 20*(x[0] + 10*x[1]) + 4*(x[1] - 2*x[2])**3, \n",
    "                     10*(x[2] - x[3]) - 8*(x[1] - 2*x[2])**3, -10*(x[2] - x[3]) - 40*(x[0] - x[3])**3])\n",
    "def hessian_func(x):\n",
    "    return np.matrix([[2 + 120*(x[0] - x[3])**2, 20, 0, -120*(x[0] - x[3])**2], \n",
    "                      [20, 200 + 12*(x[1] - 2*x[2])**2, -24*(x[1] - 2*x[2])**2, 0],\n",
    "                      [0, -24*(x[1] - 2*x[2])**2, 10 + 48*(x[1] - 2*x[2])**2, -10],\n",
    "                      [-120*(x[0] - x[3])**2, 0, -10, 10 + 120*(x[0] - x[3])**2]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration for BFGS w/ exact line search: 32\n",
      "Iteration for BFGS w/ backtracking: 58\n",
      "Iteration for SR1 w/ exact line search: 32\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-d721755e5e02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxcoords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mycoords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexact_line_search_quasi_newton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSR1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Iteration for SR1 w/ exact line search:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxcoords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mycoords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbacktracking_line_search_quasi_newton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSR1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Iteration for SR1 w/ backtracking:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-44eb5752f476>\u001b[0m in \u001b[0;36mbacktracking_line_search_quasi_newton\u001b[0;34m(update_method, x0, H0)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_k\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1e-9\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mp_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_k\u001b[0m\u001b[0;34m)\u001b[0m                \u001b[0;31m# search direction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbacktracking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_objective_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0ms_k\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mp_k\u001b[0m                     \u001b[0;31m# s_k = x_{k+1} - x_k\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mg_k1\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mgrad_objective_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_k\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms_k\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# gk1 is g_{k+1}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-4535e8374d7f>\u001b[0m in \u001b[0;36mbacktracking\u001b[0;34m(objFunc, gradObjFunc, x0, pk, alpha0, c1, rho)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0malpha\u001b[0m            \u001b[0;34m=\u001b[0m \u001b[0malpha0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# make sure it is a descent direction.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrentGradient\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mobjFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcurrentFuncValue\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mc1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrentGradient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x0 = np.array([3, -1, 0, 1])\n",
    "H0 = np.eye(4)\n",
    "\n",
    "\n",
    "\n",
    "x, iter, err, xcoords, ycoords = exact_line_search_quasi_newton(BFGS, x0, H0)\n",
    "print(\"Iteration for BFGS w/ exact line search:\", iter)\n",
    "x, iter, err, xcoords, ycoords = backtracking_line_search_quasi_newton(BFGS, x0, H0)\n",
    "print(\"Iteration for BFGS w/ backtracking:\", iter)\n",
    "x, iter, err, xcoords, ycoords = exact_line_search_quasi_newton(SR1, x0, H0)\n",
    "print(\"Iteration for SR1 w/ exact line search:\", iter)\n",
    "x, iter, err, xcoords, ycoords = backtracking_line_search_quasi_newton(SR1, x0, H0)\n",
    "print(\"Iteration for SR1 w/ backtracking:\", iter)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
